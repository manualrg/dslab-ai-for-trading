{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pprint\n",
    "\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../nb_config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "from src import utils\n",
    "from src.load_data import load_sec10k, io_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = utils.read_conf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPATH = io_utils.raw_path\n",
    "OUTFILE1 = 'metadata.pkl'\n",
    "OUTFILE2 = 'sec_10k.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download NLP Corpora\n",
    "You'll need two corpora to run this project: the stopwords corpus for removing stopwords and wordnet for lemmatizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\manua\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\manua\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get 10ks\n",
    "We'll be running NLP analysis on 10-k documents. To do that, we first need to download the documents. For this project, we'll download 10-ks for a few companies. To lookup documents for these companies, we'll use their CIK. If you would like to run this against other stocks, we've provided the dict `additional_cik` for more stocks. However, the more stocks you try, the long it will take to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AMZN': '0001018724',\n",
       " 'BMY': '0000014272',\n",
       " 'CNP': '0001130310',\n",
       " 'CVX': '0000093410',\n",
       " 'FL': '0000850209',\n",
       " 'FRT': '0000034903',\n",
       " 'HON': '0000773840'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_sec10k.cik_lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get list of 10-ks\n",
    "The SEC has a limit on the number of calls you can make to the website per second. In order to avoid hiding that limit, we've created the `SecAPI` class. This will cache data from the SEC and prevent you from going over the limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_api = load_sec10k.SecAPI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the class constructed, let's pull a list of filled 10-ks from the SEC for each company."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pull the list using the `get_sec_data` function, then display some of the results. For displaying some of the data, we'll use Amazon as an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('https://www.sec.gov/Archives/edgar/data/1018724/000101872417000011/0001018724-17-000011-index.htm',\n",
      "  '10-K',\n",
      "  '2017-02-10'),\n",
      " ('https://www.sec.gov/Archives/edgar/data/1018724/000101872416000172/0001018724-16-000172-index.htm',\n",
      "  '10-K',\n",
      "  '2016-01-29'),\n",
      " ('https://www.sec.gov/Archives/edgar/data/1018724/000101872415000006/0001018724-15-000006-index.htm',\n",
      "  '10-K',\n",
      "  '2015-01-30'),\n",
      " ('https://www.sec.gov/Archives/edgar/data/1018724/000101872414000006/0001018724-14-000006-index.htm',\n",
      "  '10-K',\n",
      "  '2014-01-31'),\n",
      " ('https://www.sec.gov/Archives/edgar/data/1018724/000119312513028520/0001193125-13-028520-index.htm',\n",
      "  '10-K',\n",
      "  '2013-01-30')]\n"
     ]
    }
   ],
   "source": [
    "example_ticker = 'AMZN'\n",
    "sec_data = {}\n",
    "\n",
    "for ticker, cik in load_sec10k.cik_lookup.items():\n",
    "    sec_data[ticker] = load_sec10k.get_sec_data(sec_api=sec_api, cik=cik, newest_pricing_data='2018-01-01', doc_type='10-K')\n",
    "\n",
    "pprint.pprint(sec_data[example_ticker][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download 10-ks\n",
    "As you see, this is a list of urls. These urls point to a file that contains metadata related to each filling. Since we don't care about the metadata, we'll pull the filling by replacing the url with the filling url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading AMZN Fillings: 100%|██████████| 22/22 [00:03<00:00,  7.09filling/s]\n",
      "Downloading BMY Fillings: 100%|██████████| 27/27 [00:04<00:00,  6.52filling/s]\n",
      "Downloading CNP Fillings: 100%|██████████| 19/19 [00:04<00:00,  3.80filling/s]\n",
      "Downloading CVX Fillings: 100%|██████████| 25/25 [00:08<00:00,  2.79filling/s]\n",
      "Downloading FL Fillings: 100%|██████████| 22/22 [00:06<00:00,  3.63filling/s]\n",
      "Downloading FRT Fillings: 100%|██████████| 29/29 [00:06<00:00,  4.25filling/s]\n",
      "Downloading HON Fillings: 100%|██████████| 25/25 [00:07<00:00,  3.32filling/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Document:\n",
      "\n",
      "<SEC-DOCUMENT>0001018724-17-000011.txt : 20170210\n",
      "<SEC-HEADER>0001018724-17-000011.hdr.sgml : 20170210\n",
      "<ACCEPTANCE-DATETIME>20170209175636\n",
      "ACCESSION NUMBER:\t\t0001018724-17-000011\n",
      "CONFORMED SUBMISSION TYPE:\t10-K\n",
      "PUBLIC DOCUMENT COUNT:\t\t92\n",
      "CONFORMED PERIOD OF REPORT:\t20161231\n",
      "FILED AS OF DATE:\t\t20170210\n",
      "DATE AS OF CHANGE:\t\t20170209\n",
      "\n",
      "FILER:\n",
      "\n",
      "\tCOMPANY DATA:\t\n",
      "\t\tCOMPANY CONFORMED NAME:\t\t\tAMAZON COM INC\n",
      "\t\tCENTRAL INDEX KEY:\t\t\t0001018724\n",
      "\t\tSTANDARD INDUSTRIAL CLASSIFICATION:\tRETAIL-CATALOG & MAIL-ORDER HOUSES [5961]\n",
      "\t\tIRS NUMBER:\t\t\t\t911646860\n",
      "\t\tSTATE OF INCORPORATION:\t\t\tDE\n",
      "\t\tFISCAL YEAR END:\t\t\t1231\n",
      "\n",
      "\tFILING VALUES:\n",
      "\t\tFORM TYPE:\t\t10-K\n",
      "\t\tSEC ACT:\t\t1934 Act\n",
      "\t\tSEC FILE NUMBER:\t000-22513\n",
      "\t\tFILM NUMBER:\t\t17588807\n",
      "\n",
      "\tBUSINESS ADDRESS:\t\n",
      "\t\tSTREET 1:\t\t410 TERRY AVENUE NORTH\n",
      "\t\tCITY:\t\t\tSEATTLE\n",
      "\t\tSTATE:\t\t\tWA\n",
      "\t\tZIP:\t\t\t98109\n",
      "\t\tBUSINESS PHONE:\t\t2062661000\n",
      "\n",
      "\tMAIL ADDRESS:\t\n",
      "\t\tSTREET 1:\t\t410 TERRY AVENUE NORTH\n",
      "\t\tCITY:\t\t\tSEATTLE\n",
      "\t\tSTATE:\t\t\tWA\n",
      "\t\tZIP:\t\t\t98109\n",
      "</SEC-HEADER>\n",
      "<DOCUMENT>\n",
      "<TYPE>10-K\n",
      "<SEQUENCE>1\n",
      "<FILENAME...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "raw_fillings_by_ticker = {}\n",
    "\n",
    "for ticker, data in sec_data.items():\n",
    "    raw_fillings_by_ticker[ticker] = {}\n",
    "    for index_url, file_type, file_date in tqdm(data, desc='Downloading {} Fillings'.format(ticker), unit='filling'):\n",
    "        if (file_type == '10-K'):\n",
    "            file_url = index_url.replace('-index.htm', '.txt').replace('.txtl', '.txt')            \n",
    "            \n",
    "            raw_fillings_by_ticker[ticker][file_date] = sec_api.get(file_url)\n",
    "\n",
    "\n",
    "print('Example Document:\\n\\n{}...'.format(next(iter(raw_fillings_by_ticker[example_ticker].values()))[:1000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Documents\n",
    "With theses fillings downloaded, we want to break them into their associated documents. These documents are sectioned off in the fillings with the tags `<DOCUMENT>` for the start of each document and `</DOCUMENT>` for the end of each document. There's no overlap with these documents, so each `</DOCUMENT>` tag should come after the `<DOCUMENT>` with no `<DOCUMENT>` tag in between.\n",
    "\n",
    "Implement `get_documents` to return a list of these documents from a filling. Make sure not to include the tag in the returned document text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `get_documents` function implemented, let's extract all the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting Documents from AMZN Fillings: 100%|██████████| 17/17 [00:00<00:00, 90.19filling/s]\n",
      "Getting Documents from BMY Fillings: 100%|██████████| 23/23 [00:00<00:00, 51.25filling/s]\n",
      "Getting Documents from CNP Fillings: 100%|██████████| 15/15 [00:00<00:00, 44.63filling/s]\n",
      "Getting Documents from CVX Fillings: 100%|██████████| 21/21 [00:00<00:00, 45.97filling/s]\n",
      "Getting Documents from FL Fillings: 100%|██████████| 16/16 [00:00<00:00, 74.96filling/s]\n",
      "Getting Documents from FRT Fillings: 100%|██████████| 19/19 [00:00<00:00, 67.54filling/s]\n",
      "Getting Documents from HON Fillings: 100%|██████████| 20/20 [00:00<00:00, 54.34filling/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0 Filed on 2017-02-10:\n",
      "\n",
      "<TYPE>10-K\n",
      "<SEQUENCE>1\n",
      "<FILENAME>amzn-20161231x10k.htm\n",
      "<DESCRIPTION>FORM 10-K\n",
      "<TEXT>\n",
      "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\">\n",
      "<html>\n",
      "\t<he...\n",
      "\n",
      "Document 1 Filed on 2017-02-10:\n",
      "\n",
      "<TYPE>EX-12.1\n",
      "<SEQUENCE>2\n",
      "<FILENAME>amzn-20161231xex121.htm\n",
      "<DESCRIPTION>COMPUTATION OF RATIO OF EARNINGS TO FIXED CHARGES\n",
      "<TEXT>\n",
      "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http:...\n",
      "\n",
      "Document 2 Filed on 2017-02-10:\n",
      "\n",
      "<TYPE>EX-21.1\n",
      "<SEQUENCE>3\n",
      "<FILENAME>amzn-20161231xex211.htm\n",
      "<DESCRIPTION>LIST OF SIGNIFICANT SUBSIDIARIES\n",
      "<TEXT>\n",
      "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/h...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "filling_documents_by_ticker = {}\n",
    "\n",
    "for ticker, raw_fillings in raw_fillings_by_ticker.items():\n",
    "    filling_documents_by_ticker[ticker] = {}\n",
    "    for file_date, filling in tqdm(raw_fillings.items(), desc='Getting Documents from {} Fillings'.format(ticker), unit='filling'):\n",
    "        filling_documents_by_ticker[ticker][file_date] = load_sec10k.get_documents(filling)\n",
    "\n",
    "\n",
    "print('\\n\\n'.join([\n",
    "    'Document {} Filed on {}:\\n{}...'.format(doc_i, file_date, doc[:200])\n",
    "    for file_date, docs in filling_documents_by_ticker[example_ticker].items()\n",
    "    for doc_i, doc in enumerate(docs)][:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Document Types\n",
    "Now that we have all the documents, we want to find the 10-k form in this 10-k filing. Implement the `get_document_type` function to return the type of document given. The document type is located on a line with the `<TYPE>` tag. For example, a form of type \"TEST\" would have the line `<TYPE>TEST`. Make sure to return the type as lowercase, so this example would be returned as \"test\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `get_document_type` function, we'll filter out all non 10-k documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    cik: '0001018724'\n",
      "    file: '\\n<TYPE>10-K\\n<SEQUENCE>1\\n<FILENAME>amzn-2016123...\n",
      "    file_date: '2017-02-10'},\n",
      "  {\n",
      "    cik: '0001018724'\n",
      "    file: '\\n<TYPE>10-K\\n<SEQUENCE>1\\n<FILENAME>amzn-2015123...\n",
      "    file_date: '2016-01-29'},\n",
      "  {\n",
      "    cik: '0001018724'\n",
      "    file: '\\n<TYPE>10-K\\n<SEQUENCE>1\\n<FILENAME>amzn-2014123...\n",
      "    file_date: '2015-01-30'},\n",
      "  {\n",
      "    cik: '0001018724'\n",
      "    file: '\\n<TYPE>10-K\\n<SEQUENCE>1\\n<FILENAME>amzn-2013123...\n",
      "    file_date: '2014-01-31'},\n",
      "  {\n",
      "    cik: '0001018724'\n",
      "    file: '\\n<TYPE>10-K\\n<SEQUENCE>1\\n<FILENAME>d445434d10k....\n",
      "    file_date: '2013-01-30'},\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "ten_ks_by_ticker = {}\n",
    "\n",
    "for ticker, filling_documents in filling_documents_by_ticker.items():\n",
    "    ten_ks_by_ticker[ticker] = []\n",
    "    for file_date, documents in filling_documents.items():\n",
    "        for document in documents:\n",
    "            if load_sec10k.get_document_type(document) == '10-k':\n",
    "                ten_ks_by_ticker[ticker].append({\n",
    "                    'cik': load_sec10k.cik_lookup[ticker],\n",
    "                    'file': document,\n",
    "                    'file_date': file_date})\n",
    "\n",
    "\n",
    "load_sec10k.print_ten_k_data(ten_ks_by_ticker[example_ticker][:5], ['cik', 'file', 'file_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {'newest_pricing_data': '2018-01-01',\n",
    "           'tickers': load_sec10k.cik_lookup}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(OUTPATH + OUTFILE1, 'wb') as file:\n",
    "    pickle.dump(metadata, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(OUTPATH + OUTFILE2, 'wb') as file:\n",
    "    pickle.dump(ten_ks_by_ticker, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
