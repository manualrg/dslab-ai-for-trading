{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\"> **Corpus Representation: SEC 10-K Fillings** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../nb_config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import pickle\n",
    "import pprint\n",
    "\n",
    "from src.nlp_quant import bow_sent\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import re\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import utils\n",
    "from src.load_data import load_sec10k, io_utils\n",
    "from src.nlp_quant import bow_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = utils.read_conf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPATH1 = os.path.join(io_utils.interim_path, 'sec_fillings', '')\n",
    "INPATH2 = os.path.join(INPATH1,'ten_ks', '')\n",
    "OUTPATH1 =  os.path.join(io_utils.interim_path, 'sec_fillings', '')\n",
    "os.path.isdir(INPATH1), os.path.isdir(INPATH2), os.path.isdir(OUTPATH1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "INFILE1 = 'metadata.pkl'\n",
    "OUTFILE1 = 'parsed_sentiment_loughran_mcdonald.csv'\n",
    "OUTFILE2 = 'tenks_risk_tfidf_by_sent.pkl'\n",
    "OUTFILE3 = 'tenks_risk_doc_len.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Sentiments vocabularies and NLTK Copora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(INPATH1 + INFILE1, 'rb') as file:\n",
    "    metadata = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\manuelalberto.romero\\AppData\\Roaming\\nltk_dat\n",
      "[nltk_data]     a...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\manuelalberto.romero\\AppData\\Roaming\\nltk_dat\n",
      "[nltk_data]     a...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_word_pattern = re.compile('\\w+')\n",
    "wlm = WordNetLemmatizer()\n",
    "lemma_english_stopwords = bow_sent.lemmatize_words(wlm, stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df = load_sec10k.get_sentiment_loughran_mcdonald()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the same preprocessing to these words as the 10-k words\n",
    "sentiment_df['word'] = bow_sent.lemmatize_words(wlm, sentiment_df['word'].str.lower())\n",
    "sentiment_df = sentiment_df.drop_duplicates('word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>uncertainty</th>\n",
       "      <th>litigious</th>\n",
       "      <th>constraining</th>\n",
       "      <th>interesting</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>absenteeism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50507</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>offense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17744</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>cybercriminals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19600</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>deprecation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47017</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>monopolization</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       negative  positive  uncertainty  litigious  constraining  interesting  \\\n",
       "251        True     False        False      False         False        False   \n",
       "50507     False     False        False       True         False        False   \n",
       "17744      True     False        False      False         False        False   \n",
       "19600      True     False        False      False         False        False   \n",
       "47017      True     False        False      False         False        False   \n",
       "\n",
       "                 word  \n",
       "251       absenteeism  \n",
       "50507         offense  \n",
       "17744  cybercriminals  \n",
       "19600     deprecation  \n",
       "47017  monopolization  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = sentiment_df.drop(columns=['word']).columns\n",
    "\n",
    "sentiments_dict = {}\n",
    "for sent_col in sentiments:\n",
    "    sentiments_dict[sent_col] = sentiment_df.loc[sentiment_df[sent_col], 'word'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply NLP Pipeline to 10Ks documents: TF-IDF Representation\n",
    "\n",
    "Pipeline steps:\n",
    "1. Tokenization\n",
    "2. Lemmatization\n",
    "3. Stop words Removal\n",
    "4. Representation: TF-IDF\n",
    "    * Compute a numerical reprentation of a corpus as a matrix, where each document is a row and each column is a vocabulary-token. Each value is a trade of between TF and IDF. Each document is characterized by a set of tokens frequency, therefore no strict semantinc relationships are captured\n",
    "    * TF: Token Frequency: Token j frequency in document i\n",
    "    * IDF: Inverse Document Frequency: A penalization on number of documents that token j appears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting tf-idf:   0%|                                                                     | 0/34 [00:00<?, ?batch/s]INFO:src.nlp_quant.bow_sent:Tickers in batch: ['abbv', 'adi', 'aee', 'abt', 'ads', 'adm', 'adp', 'aap', 'abc', 'adbe', 'aapl', 'aal', 'acn', 'adsk']\n",
      "Extracting tf-idf:   3%|█▋                                                        | 1/34 [02:14<1:14:01, 134.59s/batch]INFO:src.nlp_quant.bow_sent:Tickers in batch: ['aig', 'aet', 'aee', 'afl', 'agn', 'aiv', 'alk', 'alb', 'aep', 'aes', 'akam', 'ajg', 'algn', 'aiz']\n",
      "Extracting tf-idf:   6%|███▍                                                      | 2/34 [05:45<1:23:55, 157.36s/batch]INFO:src.nlp_quant.bow_sent:Tickers in batch: ['all', 'anss', 'amgn', 'alxn', 'antm', 'amp', 'alk', 'ame', 'alle', 'amat', 'amg', 'amd', 'amzn', 'amt']\n",
      "Extracting tf-idf:   9%|█████                                                     | 3/34 [08:52<1:25:54, 166.28s/batch]INFO:src.nlp_quant.bow_sent:Tickers in batch: ['ayi', 'are', 'apa', 'apd', 'antm', 'aos', 'avgo', 'aph', 'avb', 'awk', 'azo', 'avy', 'axp', 'atvi', 'aon', 'arnc']\n",
      "Extracting tf-idf:  12%|██████▊                                                   | 4/34 [11:09<1:18:48, 157.60s/batch]INFO:src.nlp_quant.bow_sent:Tickers in batch: ['bcr', 'a', 'bax', 'bll', 'ben', 'ba', 'bdx', 'bk', 'blk', 'azo', 'bby', 'biib', 'bac']\n",
      "Extracting tf-idf:  15%|████████▌                                                 | 5/34 [13:55<1:17:23, 160.12s/batch]INFO:src.nlp_quant.bow_sent:Tickers in batch: ['bmy', 'cb', 'cbs', 'ca', 'bll', 'cag', 'bsx', 'cboe', 'ccl', 'bxp', 'cci', 'cah', 'bwa', 'cat']\n",
      "Extracting tf-idf:  18%|██████████▏                                               | 6/34 [19:12<1:36:44, 207.29s/batch]INFO:src.nlp_quant.bow_sent:Tickers in batch: ['cmg', 'cme', 'cfg', 'chtr', 'chrw', 'cern', 'chd', 'clx', 'cma', 'ccl', 'ci', 'cmcsa', 'cinf', 'cf', 'cl']\n",
      "Extracting tf-idf:  21%|███████████▉                                              | 7/34 [24:05<1:44:49, 232.93s/batch]INFO:src.nlp_quant.bow_sent:Tickers in batch: ['cmg', 'cop', 'cnp', 'cpb', 'cmi', 'cof', 'cost', 'crm', 'coo', 'csco', 'cms', 'cog', 'cnc', 'coty']\n",
      "Extracting tf-idf:  24%|█████████████▋                                            | 8/34 [31:07<2:05:30, 289.62s/batch]INFO:src.nlp_quant.bow_sent:Tickers in batch: ['cvs', 'csx', 'ctas', 'ctxs', 'cxo', 'ctl', 'cvx', 'de', 'csco', 'c', 'dal', 'dgx', 'ctsh', 'dfs']\n",
      "Extracting tf-idf:  26%|███████████████▎                                          | 9/34 [37:30<2:12:17, 317.49s/batch]INFO:src.nlp_quant.bow_sent:Tickers in batch: ['dhi', 'dg', 'dltr', 'duk', 'dva', 'dis', 'dov', 'dlr', 'disck', 'dte', 'dish', 'dhr', 'dre', 'dgx', 'dri', 'disca']\n",
      "Extracting tf-idf:  29%|████████████████▊                                        | 10/34 [40:06<1:47:41, 269.24s/batch]INFO:src.nlp_quant.bow_sent:Tickers in batch: ['ecl', 'emr', 'dva', 'eog', 'efx', 'eix', 'emn', 'ebay', 'ed', 'd', 'ea', 'dvn', 'el']\n",
      "Extracting tf-idf:  32%|██████████████████▍                                      | 11/34 [42:37<1:29:38, 233.84s/batch]INFO:src.nlp_quant.bow_sent:Tickers in batch: ['expd', 'eog', 'expe', 'exr', 'etfc', 'es', 'ess', 'etr', 'exc', 'eqix', 'eqt', 'etn', 'ew', 'eqr']\n",
      "Extracting tf-idf:  35%|████████████████████                                     | 12/34 [45:51<1:21:20, 221.83s/batch]INFO:src.nlp_quant.bow_sent:Tickers in batch: ['fast', 'fe', 'fis', 'fbhs', 'exr', 'fitb', 'flir', 'fb', 'fcx', 'ffiv', 'fisv', 'fl', 'fdx', 'flr', 'fls']\n",
      "Extracting tf-idf:  38%|█████████████████████▊                                   | 13/34 [47:58<1:07:38, 193.27s/batch]INFO:src.nlp_quant.bow_sent:Tickers in batch: ['gm', 'glw', 'foxa', 'fmc', 'fti', 'gd', 'gpc', 'gis', 'gps', 'fox', 'fl', 'gild', 'gpn', 'f', 'frt', 'goog', 'ge']\n",
      "Extracting tf-idf:  41%|████████████████████████▎                                  | 14/34 [50:25<59:49, 179.48s/batch]INFO:src.nlp_quant.bow_sent:Tickers in batch: ['hban', 'hbi', 'hca', 'grmn', 'hig', 'gww', 'gt', 'hd', 'has', 'hes', 'hlt', 'hal', 'gs', 'gps']\n",
      "Extracting tf-idf:  44%|██████████████████████████                                 | 15/34 [53:09<55:20, 174.76s/batch]INFO:src.nlp_quant.bow_sent:Tickers in batch: ['hpq', 'hum', 'hrb', 'hon', 'hp', 'hsy', 'hsic', 'hst', 'hog', 'ibm', 'holx', 'hpe', 'hrl']\n",
      "Extracting tf-idf:  47%|███████████████████████████▊                               | 16/34 [55:12<47:46, 159.23s/batch]INFO:src.nlp_quant.bow_sent:Tickers in batch: ['incy', 'itw', 'ip', 'intc', 'ilmn', 'ipg', 'intu', 'ibm', 'info', 'ice', 'irm', 'ir', 'iff', 'idxx', 'it', 'isrg']\n",
      "Extracting tf-idf:  50%|█████████████████████████████▌                             | 17/34 [57:30<43:18, 152.84s/batch]INFO:src.nlp_quant.bow_sent:Tickers in batch: ['jci', 'key', 'jpm', 'jec', 'jbht', 'jwn', 'klac', 'jnpr', 'kmb', 'ivz', 'jnj', 'kim', 'khc', 'it']\n",
      "Extracting tf-idf:  53%|██████████████████████████████▏                          | 18/34 [1:00:12<41:28, 155.56s/batch]INFO:src.nlp_quant.bow_sent:Tickers in batch: ['lly', 'kmx', 'lb', 'lh', 'lkq', 'kss', 'ko', 'leg', 'kr', 'kmb', 'len', 'kmi', 'k', 'ksu']\n",
      "Extracting tf-idf:  56%|███████████████████████████████▊                         | 19/34 [1:02:22<36:59, 147.97s/batch]INFO:src.nlp_quant.bow_sent:Tickers in batch: ['mar', 'l', 'lmt', 'lly', 'mas', 'lnc', 'mac', 'luv', 'lrcx', 'lyb', 'maa', 'low', 'lnt']\n",
      "Extracting tf-idf:  59%|█████████████████████████████████▌                       | 20/34 [1:04:52<34:41, 148.65s/batch]INFO:src.nlp_quant.bow_sent:Tickers in batch: ['mdlz', 'mck', 'mhk', 'mkc', 'mlm', 'mat', 'ma', 'mdt', 'mas', 'met', 'mco', 'mmc', 'mgm', 'mcd', 'mchp']\n",
      "Extracting tf-idf:  62%|███████████████████████████████████▏                     | 21/34 [1:07:30<32:47, 151.38s/batch]INFO:src.nlp_quant.bow_sent:Tickers in batch: ['msft', 'ms', 'msi', 'mo', 'mpc', 'mtb', 'mnst', 'mos', 'mmc', 'mro', 'mu', 'mrk', 'mtd']\n",
      "Extracting tf-idf:  65%|████████████████████████████████████▉                    | 22/34 [1:10:10<30:47, 153.97s/batch]INFO:src.nlp_quant.bow_sent:Tickers in batch: ['m', 'nbl', 'ni', 'nke', 'noc', 'nov', 'nrg', 'myl', 'nee', 'nem', 'nsc', 'nlsn', 'mu', 'ndaq', 'navi', 'nflx']\n",
      "Extracting tf-idf:  68%|██████████████████████████████████████▌                  | 23/34 [1:13:09<29:35, 161.44s/batch]INFO:src.nlp_quant.bow_sent:Tickers in batch: ['nwl', 'oxy', 'nwsa', 'nue', 'orcl', 'nws', 'ntrs', 'orly', 'nvda', 'ntap', 'nsc', 'omc', 'o', 'oke']\n",
      "Extracting tf-idf:  71%|████████████████████████████████████████▏                | 24/34 [1:15:40<26:22, 158.22s/batch]INFO:src.nlp_quant.bow_sent:Tickers in batch: ['payx', 'pcg', 'pfg', 'peg', 'pg', 'pgr', 'pcar', 'o', 'pdco', 'pfe', 'ph', 'phm', 'pep', 'pbct']\n",
      "Extracting tf-idf:  74%|█████████████████████████████████████████▉               | 25/34 [1:17:42<22:06, 147.41s/batch]INFO:src.nlp_quant.bow_sent:Tickers in batch: ['pld', 'pki', 'pnc', 'prgo', 'psa', 'pru', 'pvh', 'pnr', 'ppl', 'pnw', 'ph', 'ppg', 'pm', 'pkg', 'psx']\n",
      "Extracting tf-idf:  76%|███████████████████████████████████████████▌             | 26/34 [1:21:15<22:17, 167.19s/batch]INFO:src.nlp_quant.bow_sent:Tickers in batch: ['rl', 'regn', 'pypl', 'reg', 'rhi', 'pwr', 'pvh', 'rf', 'rcl', 'qrvo', 'rht', 'pxd', 're', 'rjf', 'qcom']\n",
      "Extracting tf-idf:  79%|█████████████████████████████████████████████▎           | 27/34 [1:23:49<19:02, 163.23s/batch]INFO:src.nlp_quant.bow_sent:Tickers in batch: ['rl', 'rmd', 'rost', 'sbux', 'see', 'rok', 'rop', 'sbac', 'rtn', 'rsg', 'shw', 'schw', 'rrc']\n",
      "Extracting tf-idf:  82%|██████████████████████████████████████████████▉          | 28/34 [1:25:54<15:10, 151.70s/batch]INFO:src.nlp_quant.bow_sent:Tickers in batch: ['sjm', 'slg', 'spg', 'slb', 'sna', 'so', 'sni', 'sti', 'srcl', 'sig', 'shw', 'stt', 'snps', 'sre', 'stx']\n",
      "Extracting tf-idf:  85%|████████████████████████████████████████████████▌        | 29/34 [1:29:05<13:36, 163.36s/batch]INFO:src.nlp_quant.bow_sent:Tickers in batch: ['swks', 'tif', 'tjx', 'syf', 'swk', 'stz', 'tel', 'tdg', 'syy', 'tgt', 'tap', 'syk', 'stx', 'tmo']\n",
      "Extracting tf-idf:  88%|██████████████████████████████████████████████████▎      | 30/34 [1:31:20<10:19, 154.97s/batch]INFO:src.nlp_quant.bow_sent:Tickers in batch: ['trow', 'udr', 'unh', 'uaa', 'txn', 'tsn', 'txt', 't', 'ual', 'trip', 'ulta', 'uhs', 'tsco', 'trv', 'tmo']\n",
      "Extracting tf-idf:  91%|███████████████████████████████████████████████████▉     | 31/34 [1:33:27<07:19, 146.63s/batch]INFO:src.nlp_quant.bow_sent:Tickers in batch: ['var', 'unh', 'unm', 'vmc', 'ups', 'vrsk', 'vlo', 'uri', 'vno', 'vfc', 'usb', 'vrsn', 'utx', 'unp']\n",
      "Extracting tf-idf:  94%|█████████████████████████████████████████████████████▋   | 32/34 [1:35:42<04:46, 143.07s/batch]INFO:src.nlp_quant.bow_sent:Tickers in batch: ['v', 'wat', 'wfc', 'vrtx', 'wec', 'vtr', 'wba', 'wdc', 'whr', 'wmt', 'wm', 'vrsn', 'wmb', 'vz']\n",
      "Extracting tf-idf:  97%|███████████████████████████████████████████████████████▎ | 33/34 [1:37:53<02:19, 139.60s/batch]INFO:src.nlp_quant.bow_sent:Tickers in batch: ['xec', 'xel', 'yum', 'wu', 'zbh', 'zts', 'zion', 'xray', 'wy', 'wm', 'wynn', 'wrk', 'xom', 'xyl', 'xlnx', 'xrx']\n",
      "Extracting tf-idf: 100%|█████████████████████████████████████████████████████████| 34/34 [1:40:36<00:00, 177.53s/batch]\n"
     ]
    }
   ],
   "source": [
    "tf_idf_by_sent = bow_sent.batch_tfidf(inpath=INPATH2, batch_size=250,\n",
    "                                      lemmatizer=wlm, stopwords=lemma_english_stopwords, re_word_pattern=re_word_pattern,\n",
    "                                      vocabs=sentiments_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_doc_len(inpath, batch_size):\n",
    "    in_listdir = os.listdir(inpath)[:100]\n",
    "\n",
    "    n_batches = int(len(in_listdir)/batch_size)\n",
    "    in_listdir_batches = np.array_split(in_listdir, n_batches)\n",
    "    \n",
    "    doc_len_df_lst = []\n",
    "    for batch in tqdm(in_listdir_batches, desc=f'Extracting tf-idf', unit='batch'):\n",
    "        docs_meta = bow_sent.filenames_to_index(batch)\n",
    "        docs_len_lst = []\n",
    "        # Read docs and create a list of documents to process: docs_lst\n",
    "        for file in batch:\n",
    "            ticker, doc_type, date = file.split(\"_\")\n",
    "            date = date.split(\".\")[0]\n",
    "            infilename = inpath + file\n",
    "\n",
    "            with gzip.open(infilename, \"rb\") as f:\n",
    "                doc = f.read()\n",
    "            doc = doc.decode()\n",
    "            docs_len_lst.append(len(doc))  # Compute doc length\n",
    "            \n",
    "        doc_len_df_lst.append(pd.Series(index=docs_meta, data=docs_len_lst, name='doc_len'))\n",
    "        \n",
    "    return pd.concat(docs_len_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting tf-idf: 100%|██████████| 8/8 [02:41<00:00, 20.23s/batch]\n"
     ]
    }
   ],
   "source": [
    "from src.nlp_quant import bow_sent\n",
    "\n",
    "doc_lens = bow_sent.batch_doc_len(inpath=INPATH2, batch_size=1000, re_word_pattern=re_word_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ticker  date      \n",
       "aal     2003-04-15    23454\n",
       "        2004-02-27    22242\n",
       "        2005-02-25    23472\n",
       "        2006-02-24    38997\n",
       "        2007-02-23    39831\n",
       "Name: doc_len, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_lens.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write Preprocessed 10Ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df.to_csv(OUTPATH1 + OUTFILE1, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_sent.write_sent_tfidf_dict(path=OUTPATH1, name=OUTFILE2, sent_tfidf_dict=tf_idf_by_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lens.to_csv(OUTPATH1 + OUTFILE3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
