{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\"> **Donwload SEC 10-K Fillings** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import pickle\n",
    "import pprint\n",
    "\n",
    "from src.nlp_quant import bow_sent\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import re\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import utils\n",
    "from src.load_data import load_sec10k, io_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../nb_config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = utils.read_conf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOC_TYPE = '10-K'\n",
    "newest_pricing_data = '2018-01-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPATH = os.path.join(io_utils.raw_path, 'sec_10k', '')\n",
    "INFILE1 = 'metadata.pkl'\n",
    "INFILE2 = 'sec_10k.pkl'\n",
    "\n",
    "OUTFILE1 =  'parsed_sentiment_loughran_mcdonald.csv'\n",
    "os.path.isdir(INPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPATH =  os.path.join(io_utils.interim_path, 'sec_10k', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load SEC 10-K Raw fillings and NLTK Copora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(INPATH + INFILE1, 'rb') as file:\n",
    "    metadata = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(INPATH + INFILE2, 'rb') as file:\n",
    "    ten_ks_by_ticker = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load stopwords corpus for removing stopwords and wordnet for lemmatizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\manua\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\manua\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess SEc 10-Ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning AMZN 10-Ks: 100%|██████████| 17/17 [00:25<00:00,  1.51s/10-K]\n",
      "Cleaning BMY 10-Ks: 100%|██████████| 23/23 [00:56<00:00,  2.48s/10-K]\n",
      "Cleaning CNP 10-Ks: 100%|██████████| 15/15 [00:50<00:00,  3.35s/10-K]\n",
      "Cleaning CVX 10-Ks: 100%|██████████| 21/21 [01:44<00:00,  4.95s/10-K]\n",
      "Cleaning FL 10-Ks: 100%|██████████| 16/16 [00:22<00:00,  1.42s/10-K]\n",
      "Cleaning FRT 10-Ks: 100%|██████████| 19/19 [00:43<00:00,  2.31s/10-K]\n",
      "Cleaning HON 10-Ks: 100%|██████████| 20/20 [00:48<00:00,  2.42s/10-K]\n"
     ]
    }
   ],
   "source": [
    "word_pattern = re.compile('\\w+')\n",
    "wlm = WordNetLemmatizer()\n",
    "lemma_english_stopwords = bow_sent.lemmatize_words(wlm, stopwords.words('english'))\n",
    "\n",
    "for ticker, ten_ks in ten_ks_by_ticker.items():\n",
    "    for ten_k in tqdm(ten_ks, desc='Cleaning {} 10-Ks'.format(ticker), unit='10-K'):\n",
    "        # Clean html\n",
    "        ten_k['file_clean'] = bow_sent.clean_text(ten_k['file'])\n",
    "        # word lemmatization\n",
    "        ten_k['file_lemma'] = bow_sent.lemmatize_words(wlm, word_pattern.findall(ten_k['file_clean']))\n",
    "        # Remove stopwords\n",
    "        ten_k['file_lemma'] = [word for word in ten_k['file_lemma'] if word not in lemma_english_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df = bow_sent.get_sentiment_loughran_mcdonald()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the same preprocessing to these words as the 10-k words\n",
    "sentiment_df['word'] = bow_sent.lemmatize_words(wlm, sentiment_df['word'].str.lower())\n",
    "sentiment_df = sentiment_df.drop_duplicates('word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>uncertainty</th>\n",
       "      <th>litigious</th>\n",
       "      <th>constraining</th>\n",
       "      <th>interesting</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>936</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>adjournments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11766</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>chattel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21582</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>disruption</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51270</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>outage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11102</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>cautionary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       negative  positive  uncertainty  litigious  constraining  interesting  \\\n",
       "936       False     False        False       True         False        False   \n",
       "11766     False     False        False       True         False        False   \n",
       "21582      True     False        False      False         False        False   \n",
       "51270      True     False        False      False         False        False   \n",
       "11102      True     False        False      False         False        False   \n",
       "\n",
       "               word  \n",
       "936    adjournments  \n",
       "11766       chattel  \n",
       "21582    disruption  \n",
       "51270        outage  \n",
       "11102    cautionary  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write Preprocessed 10Ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df.to_csv(OUTPATH + OUTFILE1, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(OUTPATH + INFILE1, 'wb') as file:\n",
    "    pickle.dump(metadata, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(OUTPATH + INFILE2, 'wb') as file:\n",
    "    pickle.dump(ten_ks_by_ticker, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
