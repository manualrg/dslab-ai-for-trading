{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%run 04_alphaml_feat_eng.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mle_quant_utils import quant_helper, quant_factors, mle_utils\n",
    "import project_7_helper as project_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (16, 8)\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "\n",
    "# Retrieve parameters from configuration file\n",
    "with open(\"../conf.yml\", \"r\") as ymlfile:\n",
    "    cfg = yaml.load(ymlfile)\n",
    "\n",
    "OUTPATH =  \"../data/\" + cfg['output']['main'] + \"/\" + cfg['output']['alpha_factors_ml']['folder'] + \"/\"\n",
    "OUTFILE1 = cfg['output']['alpha_factors_ml']['features']\n",
    "OUTFILE2 = cfg['output']['alpha_factors_ml']['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = cfg['models']['alpha_ml']['target_col']\n",
    "splits = cfg['models']['alpha_ml']['splits']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "all_factors = pd.read_csv(OUTPATH + OUTFILE1,  parse_dates=['date'], index_col=[0,1])\n",
    "pd.concat([all_factors.head(2), all_factors.tail(2)])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "targets_df = pd.read_csv(OUTPATH + OUTFILE2, parse_dates=['date'], index_col=[0,1])\n",
    "pd.concat([all_factors.head(2), all_factors.tail(2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, X_test, y_train, y_valid, y_test = mle_utils.train_valid_test_split(all_factors, targets_df['target'],\n",
    "                                                                                      splits['train'], splits['valid'], splits['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building: Hyper Parameter Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_days = 10\n",
    "n_stocks = 500\n",
    "clf_random_state = 123\n",
    "clf_parameters = ParameterGrid({\n",
    "    'min_samples_leaf': n_stocks * np.array([10, 20]),\n",
    "    'max_features': [0.5, 0.75, 1.0],\n",
    "    'n_estimators': [250],\n",
    "    'criterion': ['entropy'],\n",
    "    'oob_score': [True],\n",
    "    'n_jobs': [-1],\n",
    "    'random_state': [clf_random_state]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from the lesson, that we’ll choose a min_samples_leaf parameter to be small enough to allow the tree to fit the data with as much detail as possible, but not so much that it overfits.  We can first propose 500, which is the number of assets in the estimation universe. Since we have about 500 stocks in the stock universe, we’ll want at least 500 stocks in a leaf for the leaf to make a prediction that is representative.  It’s common to multiply this by 2,3,5 or 10, so we’d have min samples leaf of 500, 1000, 1500, 2500, and 5000. If we were to try these values, we’d notice that the model is “too good to be true” on the training data.  A good rule of thumb for what is considered “too good to be true”, and therefore a sign of overfitting, is if the sharpe ratio is greater than 4.  Based on this, we recommend using min_sampes_leaf of 10 * 500, or 5,000.\n",
    "\n",
    "Feel free to try other values for these parameters, but also keep in mind that making too many small adjustments to hyper-parameters can lead to overfitting even the validation data, and therefore lead to less generalizable performance on the out-of-sample test set.  So when trying different parameter values, choose values that are different enough in scale (i.e. 10, 20, 100 instead of 10,11,12)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_trees in tqdm(n_trees_l, desc='Training Models', unit='Model'):\n",
    "    clf_nov = mle_utils.NoOverlapVoter(RandomForestClassifier(n_trees, **clf_parameters), n_skip_samples=4)\n",
    "    clf_nov.fit(X_train, y_train)\n",
    "    results['voting'].loc[n_trees, :] = [clf_nov.score(X_train, y_train.values), clf_nov.score(X_valid, y_valid.values), clf_nov.oob_score_]\n",
    "    models['voting'].append(clf_nov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
